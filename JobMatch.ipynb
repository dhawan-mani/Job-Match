{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c66a52e6-d1dd-4de1-9ffe-8dc7032d1049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\sunan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.0.1)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\sunan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sunan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (1.4.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.4.16-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\sunan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sunan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.5 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 5.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading regex-2024.4.16-cp310-cp310-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 268.9/268.9 kB 16.2 MB/s eta 0:00:00\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2024.4.16\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c08bddfa-d500-4188-accf-3d1cc404ac79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sunan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sunan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# NLTK library has a list of stopwords, we will use it to filter them out!\n",
    "nltk.download('stopwords')\n",
    "#The punkt module in NLTK (Natural Language Toolkit) is responsible for tokenizing text into individual words or sentences!\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "510d60dd-fdea-48ab-b882-0b9c00a0dff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['data', 'analytics', 'sql', 'tableau', 'analysis', 'business', 'insights', 'excel', 'customer', 'visualization', 'management', 'analyst', 'experience', 'enabling', 'performance', 'metrics', 'project', 'support', 'india', 'intricate', 'trends', 'requirements', 'skills', 'stakeholders', 'operational', 'process', 'product', 'market', 'team', 'key', 'jira', 'processes', 'driving', 'environment', 'ms', 'systems', 'science', 'seattle', 'wa', 'github', 'actionable', 'queries', 'bi', 'advanced', 'database', 'whether', 'behavior', 'analytical', 'using', 'interactive', 'dashboards', 'kpis', 'ensure', 'language', 'python', 'matplotlib', 'cloud', 'amazon', 'improvement', 'august', 'led', 'implementation', 'strategies', 'identify', 'revenue', 'within', 'leading', 'inventory', 'design', 'ensuring', 'seamless', 'integration', 'existing', 'proactively', 'analysts', 'technical', 'reports', 'used', 'identified', 'service', 'evaluated', 'targeted', 'development', 'analyzed', 'comprehensive', 'productivity', 'increase', 'reporting', 'portfolio', 'conducted', 'hr', 'improvements', 'university', 'himani', 'dhawan', 'executive', 'summary', 'result', 'oriented', 'extracting', 'complex', 'datasets', 'proficiency', 'allows', 'craft', 'scratch', 'uncovering', 'hidden', 'patterns', 'additionally', 'strong', 'command', 'tool', 'like', 'visualize', 'compelling', 'informative', 'manner', 'proficient', 'eliciting', 'implementing', 'tailored', 'solutions', 'synthesizing', 'writing', 'optimizing', 'handling', 'transformations', 'aggregating', 'sales', 'analyzing', 'thrive', 'diving', 'deep', 'involved', 'expertise', 'prowess', 'extends', 'beyond', 'keen', 'eye', 'detail', 'adept', 'statistical', 'techniques', 'extract', 'meaningful', 'exploratory', 'predictive', 'modeling', 'entire', 'lifecycle', 'intelligence', 'created', 'empower', 'levels', 'organization', 'presenting', 'executives', 'drilling', 'speaks', 'numpy', 'pandas', 'scipy', 'seaborn', 'postgresql', 'server', 'platforms', 'web', 'services', 'version', 'control', 'git', 'general', 'gathering', 'stakeholder', 'relationship', 'mapping', 'senior', 'feb', 'including', 'basket', 'stock', 'age', 'abc', 'clear', 'inventories', 'achieving', 'significant', 'cost', 'savings', 'enhancement', 'increased', 'storage', 'space', 'quarter', 'turnover', 'collaborated', 'closely', 'engineering', 'teams', 'develop', 'enhance', 'pipelines', 'optimal', 'new', 'pipeline', 'implementations', 'upgrades', 'infrastructure', 'worked', 'scientists', 'managers', 'vp', 'director', 'review', 'meetings', 'program', 'status', 'scripts', 'stored', 'procedures', 'views', 'perform', 'swiftly', 'resolved', 'challenges', 'optimize', 'streamline', 'operations', 'ecom', 'defined', 'monitored', 'experiments', 'understood', 'root', 'causes', 'changes', 'managed', 'four', 'junior', 'effectively', 'communicate', 'globally', 'utilizing', 'tools', 'maintaining', 'approach', 'towards', 'clearance', 'initiatives', 'implement', 'agile', 'iterative', 'facilitating', 'efficient', 'sprint', 'planning', 'task', 'tracking', 'ticketing', 'resolution', 'collaboration', 'cutting', 'delivery', 'times', 'drove', 'remarkable', 'query', 'optimization', 'tracked', 'aligning', 'organizational', 'goals', 'analyticspython', 'jan', 'jul', 'relocated', 'us', 'waiting', 'ead', 'anaqua', 'llc', 'december', 'boosted', 'efficiency', 'rigorous', 'verification', 'patent', 'streamlined', 'spanning', 'multiple', 'jurisdictions', 'performed', 'family', 'tagging', 'see', 'impact', 'across', 'addressed', 'quality', 'issues', 'software', 'platform', 'accuracy', 'reliability', 'client', 'informed', 'research', 'studies', 'analyze', 'competitor', 'assess', 'consumer', 'target', 'markets', 'acquired', 'patents', 'map', 'fit', 'lead', 'growth', 'trend', 'extracted', 'manipulated', 'databases', 'workflows', 'automated', 'alerts', 'proactive', 'response', 'changing', 'conditions', 'artech', 'information', 'october', 'february', 'acted', 'liaison', 'functions', 'translating', 'specifications', 'system', 'enhancements', 'projects', 'played', 'role', 'efficiencies', 'employee', 'retention', 'identifying', 'factors', 'informing', 'talent', 'improved', 'resource', 'allocation', 'reduction', 'idle', 'resources', 'overall', 'education', 'bachelor', 'computer', 'haryana', 'graduated', 'may', 'certifications', 'bootcamp', 'georgia', 'tech', 'aws', 'practitioner', 'foundational', 'professional', 'interests', 'natural', 'processing', 'decision', 'generative', 'ai', 'image', 'anomaly', 'detection', 'machine', 'learning', 'frameworks']\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def extract_keywords_from_pdf(pdf_file):\n",
    "    # Create a PDF reader object\n",
    "    reader = PdfReader(pdf_file)\n",
    "    \n",
    "    # Initialize an empty string to store extracted text\n",
    "    text = ''\n",
    "    \n",
    "    # Loop through each page of the PDF and extract text\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "     # Remove numbers\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Perform stemming or lemmatization if desired\n",
    "    \n",
    "    # Calculate word frequency\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    \n",
    "    # Sort words by frequency in descending order\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract top keywords (you can adjust the number as needed)\n",
    "    top_keywords = [word for word, freq in sorted_word_freq]\n",
    "    \n",
    "    return top_keywords\n",
    "\n",
    "# Example usage\n",
    "pdf_file = r'C:\\Users\\sunan\\OneDrive\\Desktop\\Himani Resumes\\Himani Dhawan.pdf'\n",
    "keywords = extract_keywords_from_pdf(pdf_file)\n",
    "print(\"Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "803aa4c2-c0be-4f18-97d9-69f239e093e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828df96-ca55-4055-823a-aa92b1c02a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8a74b0-5699-49f5-be87-d945e060803a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
